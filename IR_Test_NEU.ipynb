{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kevgam/CAS_IE_Information_Retrieval/blob/main/IR_Test_NEU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vorbereitung des Datensatzes in Spark\n",
        "\n",
        "Unser ursprünglicher Spotify-Datensatz, der über [Kaggle](https://www.kaggle.com/discussions/accomplishments/522912) verfügbar ist, umfasste fast eine Million Datensätze. Aufgrund der Grösse des Datensatzes hatten wir bei der Verarbeitung Performanceprobleme. Daher haben wir den Datensatz zunächst in der Spark-Umgebung der ZHAW vorverarbeitet.\n",
        "\n",
        "\n",
        "Nach dem Standardlogin (inkl. sc.stop() am Schluss) gemäss Anleitung der ZHAW haben wir folgende Schritte ausgeführt:"
      ],
      "metadata": {
        "id": "HXlD0sFkEwOH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation und Test der Umgebung\n",
        "\n",
        "```python\n",
        "# Installation des notwendigen Pakets\n",
        "sparky.installpackage('langdetect')\n",
        "\n",
        "# Alternativ mit pip\n",
        "pip install langdetect\n",
        "\n",
        "# Test der RDD-Funktionalität\n",
        "import os\n",
        "liste = range(16)\n",
        "rdd = sc.parallelize(liste)\n",
        "print(rdd.collect())\n",
        "print(rdd.glom().collect())\n",
        "\n",
        "# Überprüfen, ob alle Worker die notwendige Software installiert haben\n",
        "if len(list(filter(lambda x: x == [], rdd.glom().collect()))):\n",
        "    raise SystemExit(\"Nicht gut - einige Worker bleiben ohne Softwareinstallation.\")\n",
        "\n",
        "# Testfunktion für Abhängigkeiten\n",
        "def testdep(ignore_arg):\n",
        "    ip = \"160.85.252.66\"  # Beispiel-IP\n",
        "    try:\n",
        "        import lxml\n",
        "    except:\n",
        "        return f\"lxml FAILED! @ {ip}\"\n",
        "    else:\n",
        "        return f\"lxml worked @ {ip}\"\n",
        "\n",
        "# Installation von Abhängigkeiten\n",
        "import subprocess\n",
        "def installdeps(ignore_arg):\n",
        "    p = subprocess.run(\"pip install lxml\", shell=True, stdout=subprocess.PIPE)\n",
        "    return p.stdout.decode()\n",
        "\n",
        "# Ausführen der Installation und Tests\n",
        "rdd.map(installdeps).collect()\n",
        "rdd.map(testdep).collect()\n"
      ],
      "metadata": {
        "id": "aMP-0AZrFpg1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Laden und Verarbeiten der Daten\n",
        "\n",
        "```python\n",
        "# Das File wurde vorgängig in unseren Ordner auf dem Server kopiert\n",
        "filepath = 'songs_with_attributes_and_lyrics.csv'\n",
        "\n",
        "# Laden der CSV-Datei\n",
        "import pandas as pd\n",
        "dfs = pd.read_csv(filepath)\n",
        "\n",
        "# Spark DataFrame laden\n",
        "from pyspark.sql.functions import udf, col\n",
        "from pyspark.sql.types import StringType\n",
        "dfs = spark.read.csv(filepath, header=True, inferSchema=True)\n"
      ],
      "metadata": {
        "id": "jB7ejhFcF9sb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sprache erkennen und Fortschritt protokollieren\n",
        "\n",
        "```python\n",
        "from langdetect import detect\n",
        "\n",
        "# Funktion zur Erkennung der Sprache mit Fortschrittsanzeige\n",
        "def detect_language_with_progress(partition):\n",
        "    total_rows = 0\n",
        "    for row in partition:\n",
        "        try:\n",
        "            lang = detect(row['lyrics'])\n",
        "            yield (row['lyrics'], lang)  # Rückgabe: Originaltext und erkannte Sprache\n",
        "        except Exception:\n",
        "            yield (row['lyrics'], 'unknown')\n",
        "        total_rows += 1\n",
        "        if total_rows % 1000 == 0:  # Fortschritt alle 1000 Zeilen anzeigen\n",
        "            print(f\"Processed {total_rows} rows in this partition\")\n",
        "\n",
        "# RDD-Transformationen anwenden\n",
        "rdd = dfs.rdd.mapPartitions(detect_language_with_progress)\n",
        "\n",
        "# Zurück in ein DataFrame umwandeln\n",
        "schema = StringType()\n",
        "result = rdd.toDF([\"lyrics\", \"lyrics_language\"])\n",
        "\n",
        "# Fortschritt anzeigen\n",
        "result.show()\n"
      ],
      "metadata": {
        "id": "gzp-31kNGS5w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ergebnisse speichern\n",
        "\n",
        "```python\n",
        "# Als Excel-Datei speichern\n",
        "output_path = './processed_songs_with_lyrics.xlsx'\n",
        "dfs.to_excel(output_path, index=False)\n",
        "print(f\"DataFrame saved to: {output_path}\")\n",
        "\n",
        "# Als CSV-Datei speichern\n",
        "csv_output_path = './processed_songs_with_lyrics.csv'\n",
        "dfs.to_csv(csv_output_path, index=False)\n",
        "print(f\"DataFrame saved as CSV file to: {csv_output_path}\")\n"
      ],
      "metadata": {
        "id": "UBobl0MgGgdS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filterung und Speicherung der englischen Texte\n",
        "\n",
        "```python\n",
        "# Zeilen filtern, in denen die Sprache Englisch ist\n",
        "dfs_en = dfs[dfs['lyrics_language'] == 'en']\n",
        "\n",
        "# Gefilterte Daten als Excel- und CSV-Datei speichern\n",
        "dfs_en.to_excel('./processed_songs_filtered_lyrics_en.xlsx', index=False)\n",
        "dfs_en.to_csv('./processed_songs_filtered_lyrics_en.csv', index=False)\n",
        "\n",
        "print(\"Filtered DataFrame saved as 'filtered_lyrics_en.xlsx' and 'filtered_lyrics_en.csv'\")\n"
      ],
      "metadata": {
        "id": "iqCfQ1OjHDSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cbcj4nRTio8",
        "outputId": "54a50362-35a5-4d7b-a2fa-8d68214c3a39"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Herausfiltern der Datensätze ohne Albumnamen\n",
        "\n",
        "```python\n",
        "# Installation pandas und openpyxl\n",
        "!pip install pandas openpyxl\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Laden der CSV-Datei\n",
        "file_path = \"/content/drive/MyDrive/ie_information_retrieval_dataset/processed_songs_filtered_lyrics_en.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Filtern aller Zeilen, bei denen 'album_name' nicht NaN ist\n",
        "df_filtered = df.dropna(subset=['album_name'])\n",
        "\n",
        "# Speichern der Datei als CSV\n",
        "output_file_path = '/content/drive/MyDrive/ie_information_retrieval_dataset/processed_songs_filtered_lyrics_with_album_name.csv'\n",
        "df_filtered.to_csv(output_file_path, index=False)\n"
      ],
      "metadata": {
        "id": "FgxQW3nJIJy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation der benötigten Bibliotheken\n",
        "!pip install pandas openpyxl\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Lade die Excel-Datei\n",
        "file_path = \"/content/drive/MyDrive/ie_scripting_datasets/Archive/processed_songs_filtered_lyrics_en.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Entferne die Spalten 'id' und 'album_name'\n",
        "df = df.drop(columns=['id', 'album_name', 'lyrics_language'], errors='ignore')\n",
        "\n",
        "# Entferne alle Datensätze, bei denen 'duration_ms' kleiner als 240000 (4 Minuten) oder größer als 300000 (5 Minuten) ist\n",
        "df = df[(df['duration_ms'] >= 240000) & (df['duration_ms'] <= 300000)]\n",
        "\n",
        "# Entferne Sonderzeichen in den Spalten 'name' und 'artists'\n",
        "# Sonderzeichen inkl. [] werden entfernt\n",
        "df['name'] = df['name'].apply(lambda x: re.sub(r'[^\\w\\s]', '', str(x)))\n",
        "df['artists'] = df['artists'].apply(lambda x: re.sub(r'[^\\w\\s]', '', str(x)))\n",
        "\n",
        "# Filtere alle Zeilen, bei denen 'name' NaN ist\n",
        "df = df.dropna(subset=['name'])\n",
        "\n",
        "# Filtere alle Zeilen, bei denen 'artists' NaN ist\n",
        "df = df.dropna(subset=['artists'])\n",
        "\n",
        "# Entferne Duplikate basierend auf der Kombination aus 'name' und 'artists'\n",
        "df = df.drop_duplicates(subset=['name', 'artists'])\n",
        "\n",
        "# Sicherstellen, dass die Spalte 'lyrics' nur String-Werte enthält\n",
        "df = df[df['lyrics'].apply(lambda x: isinstance(x, str))]\n",
        "\n",
        "# Entferne Zeilenumbrüche innerhalb der 'lyrics' Spalte\n",
        "df['lyrics'] = df['lyrics'].apply(lambda x: str(x).replace('\\n', ' ').replace('\\r', ' ') if isinstance(x, str) else x)\n",
        "\n",
        "# Setze die 'lyrics'-Spalte in Anführungszeichen\n",
        "df['lyrics'] = df['lyrics'].apply(lambda x: f'\"{x}\"' if isinstance(x, str) else x)\n",
        "\n",
        "# Entferne Duplikate basierend auf der Kombination aus 'duration_ms' und 'lyrics'\n",
        "df = df.drop_duplicates(subset=['duration_ms', 'lyrics'])\n",
        "\n",
        "# Speichere die bereinigten Daten als CSV-Datei\n",
        "output_csv_path = '/content/drive/MyDrive/ie_scripting_datasets/Archive/processed_songs_filtered_lyrics_bereinigt.csv'\n",
        "df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "print(f\"Bereinigte CSV-Datei wurde gespeichert unter: {output_csv_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i78KzsUZOAbX",
        "outputId": "edb4cd3e-1f9a-4c42-a2ee-c5fab4ec5f57"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Bereinigte CSV-Datei wurde gespeichert unter: /content/drive/MyDrive/ie_scripting_datasets/Archive/processed_songs_filtered_lyrics_bereinigt.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import os\n",
        "\n",
        "# NLTK-Resourcen herunterladen\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# DataFrame laden\n",
        "df = pd.read_csv('/content/drive/MyDrive/ie_scripting_datasets/Archive/processed_songs_filtered_lyrics_bereinigt.csv')\n",
        "\n",
        "# Umwandlung der gesamten 'lyrics' Spalte in Strings\n",
        "df['lyrics'] = df['lyrics'].astype(str)\n",
        "\n",
        "# Funktion zur Textbereinigung\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())  # Tokenisierung & Kleinschreibung\n",
        "    tokens = [word for word in tokens if word.isalpha()]  # Sonderzeichen/Zahlen entfernen\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]  # Stopwörter entfernen\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Neue Spalte \"clean_lyrics\" erstellen, ohne die Original-Lyrics zu überschreiben\n",
        "df['clean_lyrics'] = df['lyrics'].apply(preprocess_text)\n",
        "\n",
        "# Ergebnisse speichern\n",
        "df.to_csv('/content/drive/MyDrive/ie_scripting_datasets/Archive/processed_songs_without_stopwords.csv', index=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vept6JiIC-Wy",
        "outputId": "2a16624b-77db-4599-aa9c-51b7d3368ebc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zCr_4DkMrnHu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}