{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kevgam/CAS_IE_Information_Retrieval/blob/main/IR_Test_NEU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vorbereitung des Datensatzes in Spark\n",
        "\n",
        "Unser ursprünglicher Spotify-Datensatz, der über [Kaggle](https://www.kaggle.com/discussions/accomplishments/522912) verfügbar ist, umfasste fast eine Million Datensätze. Aufgrund der Grösse des Datensatzes hatten wir bei der Verarbeitung Performanceprobleme. Daher haben wir den Datensatz zunächst in der Spark-Umgebung der ZHAW vorverarbeitet.\n",
        "\n",
        "\n",
        "Nach dem Standardlogin (inkl. sc.stop() am Schluss) gemäss Anleitung der ZHAW haben wir folgende Schritte ausgeführt:"
      ],
      "metadata": {
        "id": "HXlD0sFkEwOH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation und Test der Umgebung\n",
        "\n",
        "```python\n",
        "# Installation des notwendigen Pakets\n",
        "sparky.installpackage('langdetect')\n",
        "\n",
        "# Alternativ mit pip\n",
        "pip install langdetect\n",
        "\n",
        "# Test der RDD-Funktionalität\n",
        "import os\n",
        "liste = range(16)\n",
        "rdd = sc.parallelize(liste)\n",
        "print(rdd.collect())\n",
        "print(rdd.glom().collect())\n",
        "\n",
        "# Überprüfen, ob alle Worker die notwendige Software installiert haben\n",
        "if len(list(filter(lambda x: x == [], rdd.glom().collect()))):\n",
        "    raise SystemExit(\"Nicht gut - einige Worker bleiben ohne Softwareinstallation.\")\n",
        "\n",
        "# Testfunktion für Abhängigkeiten\n",
        "def testdep(ignore_arg):\n",
        "    ip = \"160.85.252.66\"  # Beispiel-IP\n",
        "    try:\n",
        "        import lxml\n",
        "    except:\n",
        "        return f\"lxml FAILED! @ {ip}\"\n",
        "    else:\n",
        "        return f\"lxml worked @ {ip}\"\n",
        "\n",
        "# Installation von Abhängigkeiten\n",
        "import subprocess\n",
        "def installdeps(ignore_arg):\n",
        "    p = subprocess.run(\"pip install lxml\", shell=True, stdout=subprocess.PIPE)\n",
        "    return p.stdout.decode()\n",
        "\n",
        "# Ausführen der Installation und Tests\n",
        "rdd.map(installdeps).collect()\n",
        "rdd.map(testdep).collect()\n"
      ],
      "metadata": {
        "id": "aMP-0AZrFpg1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Laden und Verarbeiten der Daten\n",
        "\n",
        "```python\n",
        "# Das File wurde vorgängig in unseren Ordner auf dem Server kopiert\n",
        "filepath = 'songs_with_attributes_and_lyrics.csv'\n",
        "\n",
        "# Laden der CSV-Datei\n",
        "import pandas as pd\n",
        "dfs = pd.read_csv(filepath)\n",
        "\n",
        "# Spark DataFrame laden\n",
        "from pyspark.sql.functions import udf, col\n",
        "from pyspark.sql.types import StringType\n",
        "dfs = spark.read.csv(filepath, header=True, inferSchema=True)\n"
      ],
      "metadata": {
        "id": "jB7ejhFcF9sb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sprache erkennen und Fortschritt protokollieren\n",
        "\n",
        "```python\n",
        "from langdetect import detect\n",
        "\n",
        "# Funktion zur Erkennung der Sprache mit Fortschrittsanzeige\n",
        "def detect_language_with_progress(partition):\n",
        "    total_rows = 0\n",
        "    for row in partition:\n",
        "        try:\n",
        "            lang = detect(row['lyrics'])\n",
        "            yield (row['lyrics'], lang)  # Rückgabe: Originaltext und erkannte Sprache\n",
        "        except Exception:\n",
        "            yield (row['lyrics'], 'unknown')\n",
        "        total_rows += 1\n",
        "        if total_rows % 1000 == 0:  # Fortschritt alle 1000 Zeilen anzeigen\n",
        "            print(f\"Processed {total_rows} rows in this partition\")\n",
        "\n",
        "# RDD-Transformationen anwenden\n",
        "rdd = dfs.rdd.mapPartitions(detect_language_with_progress)\n",
        "\n",
        "# Zurück in ein DataFrame umwandeln\n",
        "schema = StringType()\n",
        "result = rdd.toDF([\"lyrics\", \"lyrics_language\"])\n",
        "\n",
        "# Fortschritt anzeigen\n",
        "result.show()\n"
      ],
      "metadata": {
        "id": "gzp-31kNGS5w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ergebnisse speichern\n",
        "\n",
        "```python\n",
        "# Als Excel-Datei speichern\n",
        "output_path = './processed_songs_with_lyrics.xlsx'\n",
        "dfs.to_excel(output_path, index=False)\n",
        "print(f\"DataFrame saved to: {output_path}\")\n",
        "\n",
        "# Als CSV-Datei speichern\n",
        "csv_output_path = './processed_songs_with_lyrics.csv'\n",
        "dfs.to_csv(csv_output_path, index=False)\n",
        "print(f\"DataFrame saved as CSV file to: {csv_output_path}\")\n"
      ],
      "metadata": {
        "id": "UBobl0MgGgdS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filterung und Speicherung der englischen Texte\n",
        "\n",
        "```python\n",
        "# Zeilen filtern, in denen die Sprache Englisch ist\n",
        "dfs_en = dfs[dfs['lyrics_language'] == 'en']\n",
        "\n",
        "# Gefilterte Daten als Excel- und CSV-Datei speichern\n",
        "dfs_en.to_excel('./processed_songs_filtered_lyrics_en.xlsx', index=False)\n",
        "dfs_en.to_csv('./processed_songs_filtered_lyrics_en.csv', index=False)\n",
        "\n",
        "print(\"Filtered DataFrame saved as 'filtered_lyrics_en.xlsx' and 'filtered_lyrics_en.csv'\")\n"
      ],
      "metadata": {
        "id": "iqCfQ1OjHDSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Herausfiltern der Datensätze ohne Albumnamen\n",
        "\n",
        "Da die"
      ],
      "metadata": {
        "id": "FgxQW3nJIJy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cbcj4nRTio8",
        "outputId": "0ea603b4-53c3-4b68-ec36-9376cef4e3de"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation pandas und openpyxl\n",
        "!pip install pandas openpyxl\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Laden der CSV-Datei\n",
        "file_path = \"/content/drive/MyDrive/ie_information_retrieval/processed_songs_filtered_lyrics_en.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Filtere alle Zeilen, bei denen 'album_name' NaN ist\n",
        "df_filtered = df.dropna(subset=['album_name'])\n",
        "\n",
        "# Speichere das gefilterte DataFrame als CSV\n",
        "output_file_path = '/content/drive/MyDrive/ie_information_retrieval/processed_songs_filtered_lyrics_en_filtered.csv'\n",
        "df_filtered.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(f\"Die gefilterte Datei wurde gespeichert unter: {output_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "_lT4iQaCTXkl",
        "outputId": "f1538c7f-324c-4a0e-ea64-107a03f870e3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/ie_information_retrieval/processed_songs_filtered_lyrics_en.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-9417e33c68ff>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Laden der CSV-Datei\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/ie_information_retrieval/processed_songs_filtered_lyrics_en.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Filtere alle Zeilen, bei denen 'album_name' NaN ist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/ie_information_retrieval/processed_songs_filtered_lyrics_en.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrHqfWbOWQ4N",
        "outputId": "99e23df8-d187-41a3-871d-9d2d6d5f6524"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pfad zur Shapefile-Datei der Schweizer Kantonsgrenzen\n",
        "shapefile_path = \"/content/drive/MyDrive/ie_scripting_datasets/swissBOUNDARIES3D_1_5_TLM_KANTONSGEBIET.shp\"\n",
        "\n",
        "# Laden der Kantonsgrenzen und Konvertierung in das Koordinatensystem WGS84 (Latitude/Longitude)\n",
        "cantons_gdf = gpd.read_file(shapefile_path)\n",
        "if cantons_gdf.crs is None or cantons_gdf.crs.to_string() != 'EPSG:2056':\n",
        "    cantons_gdf.crs = \"EPSG:2056\"\n",
        "cantons_gdf = cantons_gdf.to_crs(epsg=4326)\n",
        "\n",
        "# Umbenennung der Kantonsnamen für das Merging\n",
        "cantons_gdf['Kanton'] = cantons_gdf['NAME'].replace({\n",
        "    'ZÃ¼rich': 'Zürich', 'GraubÃ¼nden': 'Graubünden', 'NeuchÃ¢tel': 'Neuenburg', 'GenÃ¨ve': 'Genf',\n",
        "    'Fribourg': 'Freiburg', 'Ticino': 'Tessin', 'Vaud': 'Waadt', 'Valais': 'Wallis'\n",
        "})\n"
      ],
      "metadata": {
        "id": "P4rpyg1TNLgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7SZtam0QTr-j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}