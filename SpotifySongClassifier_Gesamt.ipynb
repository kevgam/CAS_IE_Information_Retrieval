{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kevgam/CAS_IE_Information_Retrieval/blob/main/SpotifySongClassifier_Gesamt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vorbereitungsarbeiten in Spark"
      ],
      "metadata": {
        "id": "WDqJuV4MBhwl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vorbereitung des Datensatzes in Spark\n",
        "\n",
        "Unser ursprünglicher Spotify-Datensatz, der über [Kaggle](https://www.kaggle.com/discussions/accomplishments/522912) verfügbar ist, umfasste fast eine Million Datensätze. Aufgrund der Grösse des Datensatzes hatten wir bei der Verarbeitung Performanceprobleme. Daher haben wir den Datensatz zunächst in der Spark-Umgebung der ZHAW vorverarbeitet.\n",
        "\n",
        "\n",
        "Nach dem Standardlogin (inkl. sc.stop() am Schluss) gemäss Anleitung der ZHAW haben wir folgende Schritte ausgeführt:"
      ],
      "metadata": {
        "id": "HXlD0sFkEwOH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation und Test der Umgebung\n",
        "\n",
        "```python\n",
        "# Installation des notwendigen Pakets\n",
        "sparky.installpackage('langdetect')\n",
        "\n",
        "# Alternativ mit pip\n",
        "pip install langdetect\n",
        "\n",
        "# Test der RDD-Funktionalität\n",
        "import os\n",
        "liste = range(16)\n",
        "rdd = sc.parallelize(liste)\n",
        "print(rdd.collect())\n",
        "print(rdd.glom().collect())\n",
        "\n",
        "# Überprüfen, ob alle Worker die notwendige Software installiert haben\n",
        "if len(list(filter(lambda x: x == [], rdd.glom().collect()))):\n",
        "    raise SystemExit(\"Nicht gut - einige Worker bleiben ohne Softwareinstallation.\")\n",
        "\n",
        "# Testfunktion für Abhängigkeiten\n",
        "def testdep(ignore_arg):\n",
        "    ip = \"160.85.252.66\"  # Beispiel-IP\n",
        "    try:\n",
        "        import lxml\n",
        "    except:\n",
        "        return f\"lxml FAILED! @ {ip}\"\n",
        "    else:\n",
        "        return f\"lxml worked @ {ip}\"\n",
        "\n",
        "# Installation von Abhängigkeiten\n",
        "import subprocess\n",
        "def installdeps(ignore_arg):\n",
        "    p = subprocess.run(\"pip install lxml\", shell=True, stdout=subprocess.PIPE)\n",
        "    return p.stdout.decode()\n",
        "\n",
        "# Ausführen der Installation und Tests\n",
        "rdd.map(installdeps).collect()\n",
        "rdd.map(testdep).collect()\n"
      ],
      "metadata": {
        "id": "aMP-0AZrFpg1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Laden und Verarbeiten der Daten\n",
        "\n",
        "```python\n",
        "# Das File wurde vorgängig in unseren Ordner auf dem Server kopiert\n",
        "filepath = 'songs_with_attributes_and_lyrics.csv'\n",
        "\n",
        "# Laden der CSV-Datei\n",
        "import pandas as pd\n",
        "dfs = pd.read_csv(filepath)\n",
        "\n",
        "# Spark DataFrame laden\n",
        "from pyspark.sql.functions import udf, col\n",
        "from pyspark.sql.types import StringType\n",
        "dfs = spark.read.csv(filepath, header=True, inferSchema=True)\n"
      ],
      "metadata": {
        "id": "jB7ejhFcF9sb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sprache erkennen und Fortschritt protokollieren\n",
        "\n",
        "```python\n",
        "from langdetect import detect\n",
        "\n",
        "# Funktion zur Erkennung der Sprache mit Fortschrittsanzeige\n",
        "def detect_language_with_progress(partition):\n",
        "    total_rows = 0\n",
        "    for row in partition:\n",
        "        try:\n",
        "            lang = detect(row['lyrics'])\n",
        "            yield (row['lyrics'], lang)  # Rückgabe: Originaltext und erkannte Sprache\n",
        "        except Exception:\n",
        "            yield (row['lyrics'], 'unknown')\n",
        "        total_rows += 1\n",
        "        if total_rows % 1000 == 0:  # Fortschritt alle 1000 Zeilen anzeigen\n",
        "            print(f\"Processed {total_rows} rows in this partition\")\n",
        "\n",
        "# RDD-Transformationen anwenden\n",
        "rdd = dfs.rdd.mapPartitions(detect_language_with_progress)\n",
        "\n",
        "# Zurück in ein DataFrame umwandeln\n",
        "schema = StringType()\n",
        "result = rdd.toDF([\"lyrics\", \"lyrics_language\"])\n",
        "\n",
        "# Fortschritt anzeigen\n",
        "result.show()\n"
      ],
      "metadata": {
        "id": "gzp-31kNGS5w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ergebnisse speichern\n",
        "\n",
        "```python\n",
        "# Als Excel-Datei speichern\n",
        "output_path = './processed_songs_with_lyrics.xlsx'\n",
        "dfs.to_excel(output_path, index=False)\n",
        "print(f\"DataFrame saved to: {output_path}\")\n",
        "\n",
        "# Als CSV-Datei speichern\n",
        "csv_output_path = './processed_songs_with_lyrics.csv'\n",
        "dfs.to_csv(csv_output_path, index=False)\n",
        "print(f\"DataFrame saved as CSV file to: {csv_output_path}\")\n"
      ],
      "metadata": {
        "id": "UBobl0MgGgdS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filterung und Speicherung der englischen Texte\n",
        "\n",
        "```python\n",
        "# Zeilen filtern, in denen die Sprache Englisch ist\n",
        "dfs_en = dfs[dfs['lyrics_language'] == 'en']\n",
        "\n",
        "# Gefilterte Daten als Excel- und CSV-Datei speichern\n",
        "dfs_en.to_excel('./processed_songs_filtered_lyrics_en.xlsx', index=False)\n",
        "dfs_en.to_csv('./processed_songs_filtered_lyrics_en.csv', index=False)\n",
        "\n",
        "print(\"Filtered DataFrame saved as 'filtered_lyrics_en.xlsx' and 'filtered_lyrics_en.csv'\")\n"
      ],
      "metadata": {
        "id": "iqCfQ1OjHDSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aufbereitung der Daten für Spotify Song Classifer"
      ],
      "metadata": {
        "id": "nXuP93L2B0xq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google Drive verbinden"
      ],
      "metadata": {
        "id": "Oh7Mxlu5B90c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cbcj4nRTio8",
        "outputId": "68564dc6-9284-4217-da16-30dcb7fd9aac"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation der Python-Pakete"
      ],
      "metadata": {
        "id": "qUbCmuCBCpLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation\n",
        "!pip install pandas openpyxl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPm1lOoKlgCj",
        "outputId": "080b84c2-ccfb-4004-f106-0f9f3debf37b",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import der Bibliotheken"
      ],
      "metadata": {
        "id": "3cwuA_RQC96H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Für Datenbereinigung\n",
        "import pandas as pd\n",
        "\n",
        "# Für das Parsen und Analysieren\n",
        "import ast\n",
        "\n",
        "# Für das Zählen von Elementen\n",
        "from collections import Counter\n",
        "\n",
        "# Für die Erstellung von Visualisierungen\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Für natürliche Sprachverarbeitung (Natural Language Processing)\n",
        "import nltk\n",
        "\n",
        "# Importiere die 'stopwords'-Liste und die Funktion um Text in einzelne Wörter zu zerlegen\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Für die Tokenisierung von Texten und Entfernung von Stopwörtern\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Für reguläre Ausdrücke für die Textverarbeitung\n",
        "import re"
      ],
      "metadata": {
        "id": "YenGP5yDCw6C",
        "collapsed": true,
        "outputId": "d4ced7b4-e809-4518-989d-e0500291196e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Laden der Datei, Filtern und Datenbereinigungen"
      ],
      "metadata": {
        "id": "C8W-MnwSEmCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Laden der Excel-Datei\n",
        "file_path = \"/content/drive/MyDrive/ie_scripting_datasets/Archive/processed_songs_filtered_lyrics_en.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Entfernung der Spalten 'id' und 'album_name'\n",
        "df = df.drop(columns=['id', 'album_name', 'lyrics_language'], errors='ignore')\n",
        "\n",
        "# Entfernung aller Datensätze, bei denen 'duration_ms' kleiner als 240000 (4 Minuten) oder größer als 300000 (5 Minuten) ist\n",
        "df = df[(df['duration_ms'] >= 240000) & (df['duration_ms'] <= 300000)]\n",
        "\n",
        "# Entfernung der Sonderzeichen in den Spalten 'name' und 'artists'\n",
        "# Sonderzeichen inkl. [] werden entfernt\n",
        "df['name'] = df['name'].apply(lambda x: re.sub(r'[^\\w\\s]', '', str(x)))\n",
        "df['artists'] = df['artists'].apply(lambda x: re.sub(r'[^\\w\\s]', '', str(x)))\n",
        "\n",
        "# Filterung aller Zeilen, bei denen 'name' NaN ist\n",
        "df = df.dropna(subset=['name'])\n",
        "\n",
        "# Filterung aller Zeilen, bei denen 'artists' NaN ist\n",
        "df = df.dropna(subset=['artists'])\n",
        "\n",
        "# Entfernung Duplikate basierend auf der Kombination aus 'name' und 'artists'\n",
        "df = df.drop_duplicates(subset=['name', 'artists'])\n",
        "\n",
        "# Sicherstellen, dass die Spalte 'lyrics' nur String-Werte enthält\n",
        "df = df[df['lyrics'].apply(lambda x: isinstance(x, str))]\n",
        "\n",
        "# Entfernung der Zeilenumbrüche innerhalb der 'lyrics' Spalte\n",
        "df['lyrics'] = df['lyrics'].apply(lambda x: str(x).replace('\\n', ' ').replace('\\r', ' ') if isinstance(x, str) else x)\n",
        "\n",
        "# Anführungszeichen zur Texterkennung in der 'lyrics'-Spalte\n",
        "df['lyrics'] = df['lyrics'].apply(lambda x: f'\"{x}\"' if isinstance(x, str) else x)\n",
        "\n",
        "# Entfernung der Duplikate basierend auf der Kombination aus 'duration_ms' und 'lyrics'\n",
        "df = df.drop_duplicates(subset=['duration_ms', 'lyrics'])\n",
        "\n",
        "# Speicherung der bereinigten Daten als CSV-Datei\n",
        "output_csv_path = '/content/drive/MyDrive/ie_scripting_datasets/Archive/processed_songs_filtered_lyrics_bereinigt.csv'\n",
        "df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "print(f\"Bereinigte CSV-Datei wurde gespeichert unter: {output_csv_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 526
        },
        "id": "i78KzsUZOAbX",
        "outputId": "084ef762-eb49-4dc6-a72b-4369e58dc76d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-4ec661419f50>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Lade die Excel-Datei\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/ie_scripting_datasets/Archive/processed_songs_filtered_lyrics_en.xlsx\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Entferne die Spalten 'id' und 'album_name'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         io = ExcelFile(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1551\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1405\u001b[0m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPEEK_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stream is empty\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bereinigung der Songtexte (Tokenisierung, Kleinschreibung, Entfernung Sonderzeichen und Stopwörter)"
      ],
      "metadata": {
        "id": "iKUaHq6jE1X1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Laden des Dataframes\n",
        "df = pd.read_csv('/content/drive/MyDrive/ie_scripting_datasets/Archive/processed_songs_filtered_lyrics_bereinigt.csv')\n",
        "\n",
        "# Sicherstellen, dass die Spalte 'lyrics' keine fehlenden Werte enthält und Strings sind\n",
        "df['lyrics'] = df['lyrics'].fillna('').astype(str)\n",
        "\n",
        "# Funktion zur Textbereinigung\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):  # Sicherstellen, dass die Eingabe ein String ist\n",
        "        text = str(text)\n",
        "    tokens = word_tokenize(text.lower())  # Tokenisierung & Kleinschreibung\n",
        "    tokens = [word for word in tokens if word.isalpha()]  # Sonderzeichen/Zahlen entfernen\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]  # Stopwörter entfernen\n",
        "    return tokens  # Rückgabe als Liste von Tokens\n",
        "\n",
        "# Bereinigte Tokens in neuer Spalte speichern\n",
        "df['clean_lyrics'] = df['lyrics'].apply(preprocess_text)\n",
        "\n",
        "# Ergebnisse speichern\n",
        "df.to_csv('/content/drive/MyDrive/ie_scripting_datasets/Archive/processed_songs_without_stopwords.csv', index=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vept6JiIC-Wy",
        "outputId": "b5b9b1a9-094e-49b2-e7de-a0d3bb087062"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entfernung Duplikate und NaN-Werte in Lyrics"
      ],
      "metadata": {
        "id": "x08zY0IUFKuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Laden des Dataframes\n",
        "df = pd.read_csv('/content/drive/MyDrive/ie_scripting_datasets/Archive/processed_songs_without_stopwords.csv')\n",
        "\n",
        "# Entferne Duplikate in 'clean_lyrics'\n",
        "df = df.drop_duplicates(['clean_lyrics'])\n",
        "\n",
        "# Filtere alle Zeilen, bei denen 'clean_lyrics' NaN ist\n",
        "df = df.dropna(subset=['clean_lyrics'])\n",
        "\n",
        "# Ergebnisse speichern\n",
        "df.to_csv('/content/drive/MyDrive/ie_scripting_datasets/Archive/processed_songs_without_duplicates.csv', index=False)\n"
      ],
      "metadata": {
        "id": "zCr_4DkMrnHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zuordnung der Emotionen auf Basis der Songtexte mittels NRC Emotion Lexikon und der Spotify-Metadaten\n",
        "\n",
        "\n",
        "### Das NRC Emotion Lexikon ordnet Wörter bestimmten Emotionen (z. B. Freude, Traurigkeit, Angst) sowie positiven oder negativen Stimmungen zu.\n",
        "\n",
        "### Das Lexikon ist unter folgendem Link verfügbar:\n",
        "https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm"
      ],
      "metadata": {
        "id": "hYeIZKo5GTNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Daten laden\n",
        "data = pd.read_csv('/content/drive/MyDrive/ie_scripting_datasets/Archive/processed_songs_without_duplicates.csv')\n",
        "\n",
        "\n",
        "# Lade das NRC Emotion Lexikon\n",
        "nrc_lexicon = pd.read_csv(\n",
        "    '/content/drive/MyDrive/ie_scripting_datasets/Archive/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt',\n",
        "    sep='\\t',\n",
        "    header=None,\n",
        "    names=['word', 'emotion', 'value']\n",
        ")\n",
        "\n",
        "\n",
        "# Nur relevante Einträge aus dem NRC Emotion Lexikon behalten\n",
        "nrc_lexicon = nrc_lexicon[nrc_lexicon['value'] == 1]\n",
        "\n",
        "emotion_categories = {\n",
        "    'anger': ['anger'],\n",
        "    'fear': ['fear'],\n",
        "    'anticipation': ['anticipation'],\n",
        "    'trust': ['trust'],\n",
        "    'surprise': ['surprise'],\n",
        "    'sadness': ['sadness'],\n",
        "    'joy': ['joy'],\n",
        "    'disgust': ['disgust']\n",
        "}\n",
        "\n",
        "\n",
        "# Emotionen aus den Songtexten extrahieren\n",
        "def get_emotions_and_category(tokens, lexicon, emotion_categories):\n",
        "\n",
        "    # Überprüfung, ob die Eingabe ein String ist und Umwandlung in eine Liste von Tokens\n",
        "    if isinstance(tokens, str):\n",
        "        tokens = ast.literal_eval(tokens)\n",
        "\n",
        "    # Filterung der Wörter im Lexikon, die in den Tokens vorkommen und Extraktion der zugehörenden Emotionen\n",
        "    emotions = lexicon[lexicon['word'].isin(tokens)]['emotion'].values\n",
        "\n",
        "    # Zählung Häufigkeit jeder Emotion\n",
        "    emotion_counts = Counter(emotions)\n",
        "\n",
        "    # Summe je Emotion-Kategorie\n",
        "    category_counts = {\n",
        "        category: sum(emotion_counts[emotion] for emotion in emotions_list)\n",
        "        for category, emotions_list in emotion_categories.items()\n",
        "    }\n",
        "\n",
        "    # Bestimmung, ob die Stimmung positiv oder negativ ist, basierend auf den 'joy'- und 'sadness'-Werten\n",
        "    is_positive = category_counts.get('joy', 0) > category_counts.get('sadness', 0)\n",
        "    sentiment = 'positive' if is_positive else 'negative'\n",
        "\n",
        "    # Resultate\n",
        "    return emotion_counts, category_counts, sentiment\n",
        "\n",
        "\n",
        "# Anwendung der Funktion auf die Spalte 'clean_lyrics'\n",
        "# Berechnung der Emotions- und Kategoriewerte sowie des Sentiments je Songtext\n",
        "results = data['clean_lyrics'].apply(\n",
        "    lambda x: get_emotions_and_category(x, nrc_lexicon, emotion_categories)\n",
        ")\n",
        "\n",
        "\n",
        "# Ergänzung der Resultate in die Tabelle:\n",
        "data['emotion_counts'] = results.map(lambda x: x[0])\n",
        "data['category_counts'] = results.map(lambda x: x[1])\n",
        "data['primary_emotion'] = results.map(lambda x: max(x[1], key=x[1].get) if x[1] else 'neutral')\n",
        "\n",
        "\n",
        "# Funktion für die Ableitung der Emotionen aus den Spotify-Metadaten\n",
        "# Die Metadaten werden verwendet, um Punktzahlen für verschiedene Emotionen zu berechnen.\n",
        "# Die Emotion mit den höchsten Punkten wird als primäre Emotion ausgewählt.\n",
        "\n",
        "def determine_emotion_from_spotify_features(row):\n",
        "    danceability = row['danceability']\n",
        "    energy = row['energy']\n",
        "    loudness = row['loudness']\n",
        "    valence = row['valence']\n",
        "    tempo = row['tempo']\n",
        "    acousticness = row['acousticness']\n",
        "\n",
        "    # Berechnung der Punkte\n",
        "    emotion_scores = {\n",
        "        'joy': (valence * 0.4) + (danceability * 0.3) + (tempo * 0.3),\n",
        "        'sadness': (1 - valence) * 0.5 + (acousticness * 0.5),\n",
        "        'anger': (energy * 0.6) + (loudness * 0.4),\n",
        "        'trust': (acousticness * 0.7) + (danceability * 0.3),\n",
        "        'fear': (1 - energy) * 0.6 + (1 - loudness) * 0.4,\n",
        "        'surprise': (tempo * 0.5) + (row['liveness'] * 0.5)\n",
        "    }\n",
        "\n",
        "    # Resultate\n",
        "    return max(emotion_scores, key=emotion_scores.get)\n",
        "\n",
        "# Anwendung der Funktion auf den gesamten Datensatz, neue Spalte mit der primären Emotion\n",
        "data['primary_emotion_from_features'] = data.apply(determine_emotion_from_spotify_features, axis=1)\n",
        "\n",
        "\n",
        "\n",
        "# Funktion für die Kombination der Emotionen\n",
        "# Bei gleicher Emotion wird diese übernommen. Bei unterschiedlichen erfolgt die Zuordnung aufgrund der Gewichtung.\n",
        "def combine_emotions(lyrics_emotion, features_emotion):\n",
        "    # Überprüfung, ob die Emotionen aus den Songtexten und den Spotify-Merkmalen übereinstimmen\n",
        "    if lyrics_emotion == features_emotion:\n",
        "        return lyrics_emotion\n",
        "    else:\n",
        "        # Gewichtung für jede Emotion basierend auf ihrer Bedeutung\n",
        "        emotion_weights = {\n",
        "            'joy': 3,\n",
        "            'sadness': 3,\n",
        "            'anger': 2,\n",
        "            'fear': 2,\n",
        "            'trust': 1,\n",
        "            'disgust': 1,\n",
        "            'anticipation': 1,\n",
        "            'surprise': 1\n",
        "        }\n",
        "        # Aufruf der Gewichtung für die Emotionen aus den Songtexten und den Spotify-Merkmalen\n",
        "        lyrics_weight = emotion_weights.get(lyrics_emotion, 0)\n",
        "        features_weight = emotion_weights.get(features_emotion, 0)\n",
        "        # Gibt die Emotion mit der höheren Gewichtung zurück. Bei Gleichstand wird die Emotion aus den Songtexten übernommen.\n",
        "        return lyrics_emotion if lyrics_weight >= features_weight else features_emotion\n",
        "\n",
        "# Anwendung der Funkion auf alle Songs, Erstellung der Spalte \"final_emotion\"\n",
        "data['final_emotion'] = data.apply(\n",
        "    lambda row: combine_emotions(row['primary_emotion'], row['primary_emotion_from_features']), axis=1\n",
        ")\n",
        "\n",
        "\n",
        "# Speichern der Ergebnisse\n",
        "output_csv_path = '/content/drive/MyDrive/ie_scripting_datasets/Archive/songs_with_combined_emotions.csv'\n",
        "data.to_csv(output_csv_path, index=False)\n",
        "print(f\"Ergebnisse gespeichert: {output_csv_path}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "d1YRAFXM-EQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualisierung und Suchfunktion für Spotify Song Classifier / Auch als separates Notebook gespeichert"
      ],
      "metadata": {
        "id": "cIHPtT45ltZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation der Python-Pakete"
      ],
      "metadata": {
        "id": "64kDsGyGissx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation\n",
        "\n",
        "!pip install ipywidgets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29uX7b05Nbi6",
        "outputId": "21f7d16f-a221-4244-b74e-be6e3dd16fe1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (7.7.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (3.6.10)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (3.0.13)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.3.3)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.5.5)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.4)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (24.0.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.7.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (7.16.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.21.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.3.6)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.12.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.2)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.10.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (24.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.4.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.23.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.1->jupyter-client->ipykernel>=4.5.1->ipywidgets) (1.17.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.22.3)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.24.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.22)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.2.2)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi\n",
            "Successfully installed jedi-0.19.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import der Bibliotheken"
      ],
      "metadata": {
        "id": "14wiMftWmBJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Für interaktive Widgets\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# Für natürliche Sprachverarbeitung (Natural Language Processing)\n",
        "import nltk\n",
        "\n",
        "# Importiere die 'stopwords'-Liste und die Funktion um Text in einzelne Wörter zu zerlegen\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "# Für die Tokenisierung von Texten und Entfernung von Stopwörtern\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "# Für die Anzeige von Grafiken und die Datenmanipulation\n",
        "from IPython.display import display, clear_output\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Für reguläre Ausdrücke für die Textverarbeitung\n",
        "import re"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jbufOeG3irLv",
        "outputId": "deefde98-299d-4e52-c618-f9d3b67d1f72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google Drive verbinden"
      ],
      "metadata": {
        "id": "W4eqSMTzCZ4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "zkMwPhgnzTc-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "579e3abf-df38-4934-87ff-5a1a42953970",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interaktive Suche mit ipywidgets"
      ],
      "metadata": {
        "id": "9N34GYtTN0-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Laden der CSV-Datei mit den bereinigten Song-Daten\n",
        "data = pd.read_csv('/content/drive/MyDrive/ie_scripting_datasets/Archive/songs_with_combined_emotions.csv')\n",
        "\n",
        "\n",
        "\n",
        "# Funktion zur Visualisierung und Anzeige der Songs nach Emotion\n",
        "def visualize_emotions_and_display_songs(filtered_data, title=\"Distribution by Emotion\"):\n",
        "    \"\"\"\n",
        "    Diese Funktion visualisiert die Verteilung der Emotionen und zeigt die gefilterten Songs an.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8, 5))\n",
        "\n",
        "    # Definition der Farben je Emotion\n",
        "    emotion_colors = {\n",
        "        'anger': '#FF5733', 'fear': '#C70039', 'anticipation': '#FFC300',\n",
        "        'trust': '#DAF7A6', 'surprise': '#900C3F', 'sadness': '#3498DB',\n",
        "        'joy': '#2ECC71', 'disgust': '#6C3483'\n",
        "    }\n",
        "\n",
        "    # Anwendung der Farben\n",
        "    palette = [emotion_colors.get(emotion, '#7F8C8D') for emotion in filtered_data['final_emotion'].unique()]\n",
        "\n",
        "    # Erstellung des Balkendiagramm\n",
        "    sns.countplot(\n",
        "        data=filtered_data,\n",
        "        x='final_emotion',\n",
        "        order=filtered_data['final_emotion'].value_counts().index,\n",
        "        hue='final_emotion',\n",
        "        palette=palette,\n",
        "        legend=False\n",
        "    )\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Emotion')\n",
        "    plt.ylabel('Number of Songs')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "    # Umbenennen der Spalten\n",
        "    filtered_data = filtered_data.rename(columns={\n",
        "        'name': 'Songname', 'artists': 'Artist', 'final_emotion': 'Emotion',\n",
        "        'valence': 'Valence', 'danceability': 'Danceability', 'energy': 'Energy',\n",
        "        'loudness': 'Loudness', 'tempo': 'Tempo', 'lyrics': 'Lyrics'\n",
        "    })\n",
        "\n",
        "    # Anzeige der gefilterten Songs\n",
        "    if not filtered_data.empty:\n",
        "        display(filtered_data[['Songname', 'Artist', 'Emotion', 'Valence',\n",
        "                               'Danceability', 'Energy', 'Loudness', 'Tempo', 'Lyrics']])\n",
        "    else:\n",
        "        print(\"No songs found. Please adjust the filters.\")\n",
        "\n",
        "\n",
        "\n",
        "# Definieren der Dropdown-Menüs und Schieberegler für die verschiedenen Filter\n",
        "\n",
        "emotion_dropdown = widgets.Dropdown(\n",
        "    options=['All'] + sorted(data['final_emotion'].unique()),  # Alle Emotionen als Optionen\n",
        "    value='All',  # Standardwert: Alle Emotionen\n",
        "    description='Emotion:',\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "# Wenn der Künstler unbekannt ist, setze ihn auf 'Unknown'\n",
        "data['artists'] = data['artists'].fillna('Unknown').astype(str)\n",
        "artist_dropdown = widgets.Dropdown(\n",
        "    options=['All'] + sorted(data['artists'].unique()),  # Alle Künstler als Optionen\n",
        "    value='All',  # Standardwert: Alle Künstler\n",
        "    description='Artist:',\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "# Schieberegler für die Spotify-Metriken\n",
        "valence_slider = widgets.FloatRangeSlider(\n",
        "    value=[0.0, 1.0], min=0.0, max=1.0, step=0.01,\n",
        "    description='Valence:', style={'description_width': 'initial'}, layout={'width': '30%'}\n",
        ")\n",
        "\n",
        "danceability_slider = widgets.FloatRangeSlider(\n",
        "    value=[0.0, 1.0], min=0.0, max=1.0, step=0.01,\n",
        "    description='Danceability:', style={'description_width': 'initial'}, layout={'width': '30%'}\n",
        ")\n",
        "\n",
        "energy_slider = widgets.FloatRangeSlider(\n",
        "    value=[0.0, 1.0], min=0.0, max=1.0, step=0.01,\n",
        "    description='Energy:', style={'description_width': 'initial'}, layout={'width': '30%'}\n",
        ")\n",
        "\n",
        "loudness_slider = widgets.FloatRangeSlider(\n",
        "    value=[-60, 0], min=-60, max=0, step=0.5,\n",
        "    description='Loudness:', style={'description_width': 'initial'}, layout={'width': '30%'}\n",
        ")\n",
        "\n",
        "tempo_slider = widgets.FloatRangeSlider(\n",
        "    value=[50, 200], min=50, max=200, step=1,\n",
        "    description='Tempo:', style={'description_width': 'initial'}, layout={'width': '30%'}\n",
        ")\n",
        "\n",
        "# Textfeld zur Lyrics-Suche\n",
        "lyrics_search = widgets.Text(\n",
        "    value='', placeholder='Enter text or words',\n",
        "    description='Lyrics:', style={'description_width': 'initial'}, layout={'width': '30%'}\n",
        ")\n",
        "\n",
        "# Filter-Button, der die Anwendung der Filter auslöst\n",
        "filter_button = widgets.Button(\n",
        "    description='Filter', button_style='success', icon='filter'\n",
        ")\n",
        "\n",
        "output = widgets.Output()  # Ausgabe-Widget für gefilterte Ergebnisse\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Bereinigung der Lyrics-Sucheingabe\n",
        "def preprocess_search_text(text):\n",
        "    \"\"\"\n",
        "    Diese Funktion bereinigt den eingegebenen Suchtext:\n",
        "    - Wandelt den Text in Kleinbuchstaben um\n",
        "    - Tokenisiert den Text\n",
        "    - Entfernt Zahlen und Sonderzeichen\n",
        "    - Entfernt Stopwörter\n",
        "    \"\"\"\n",
        "    tokens = word_tokenize(text.lower())  # Tokenisierung & Kleinschreibung\n",
        "    tokens = [word for word in tokens if word.isalpha()]  # Sonderzeichen und Zahlen entfernen\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]  # Stopwörter entfernen\n",
        "    return tokens\n",
        "\n",
        "\n",
        "\n",
        "# Filter-Funktion\n",
        "def apply_filters(change=None):\n",
        "    \"\"\"\n",
        "    Diese Funktion filtert die Daten basierend auf den Benutzereingaben.\n",
        "    - Filtert nach Emotionen, Künstlern, Spotify-Metriken und Lyrics.\n",
        "    \"\"\"\n",
        "    with output:\n",
        "        clear_output(wait=True)  # Vorherige Ausgaben löschen\n",
        "        filtered_data = data.copy()  # Originaldaten kopieren, um Filter anzuwenden\n",
        "\n",
        "        # Emotionen filtern\n",
        "        if emotion_dropdown.value != 'All':\n",
        "            filtered_data = filtered_data[filtered_data['final_emotion'] == emotion_dropdown.value]\n",
        "\n",
        "        # Künstler filtern\n",
        "        if artist_dropdown.value != 'All':\n",
        "            filtered_data = filtered_data[filtered_data['artists'] == artist_dropdown.value]\n",
        "\n",
        "        # Stimmung filtern\n",
        "        min_valence, max_valence = valence_slider.value\n",
        "        filtered_data = filtered_data[\n",
        "            (filtered_data['valence'] >= min_valence) & (filtered_data['valence'] <= max_valence)\n",
        "        ]\n",
        "\n",
        "        # Tanzbarkeit filtern\n",
        "        min_danceability, max_danceability = danceability_slider.value\n",
        "        filtered_data = filtered_data[\n",
        "            (filtered_data['danceability'] >= min_danceability) & (filtered_data['danceability'] <= max_danceability)\n",
        "        ]\n",
        "\n",
        "        # Energie filtern\n",
        "        min_energy, max_energy = energy_slider.value\n",
        "        filtered_data = filtered_data[\n",
        "            (filtered_data['energy'] >= min_energy) & (filtered_data['energy'] <= max_energy)\n",
        "        ]\n",
        "\n",
        "        # Lautstärke filtern\n",
        "        min_loudness, max_loudness = loudness_slider.value\n",
        "        filtered_data = filtered_data[\n",
        "            (filtered_data['loudness'] >= min_loudness) & (filtered_data['loudness'] <= max_loudness)\n",
        "        ]\n",
        "\n",
        "        # Tempo filtern\n",
        "        min_tempo, max_tempo = tempo_slider.value\n",
        "        filtered_data = filtered_data[\n",
        "            (filtered_data['tempo'] >= min_tempo) & (filtered_data['tempo'] <= max_tempo)\n",
        "        ]\n",
        "\n",
        "        # Songtext-Suche\n",
        "        if lyrics_search.value.strip():\n",
        "            # Bereinigung der Suchworteingabe\n",
        "            search_tokens = preprocess_search_text(lyrics_search.value.strip())\n",
        "            print(f\"Such-Token: {search_tokens}\")  # Anzeige der Tokens, die in den Songtexten gesucht werden\n",
        "            if search_tokens:\n",
        "                # Filtern: Alle Tokens müssen in den Songtexten enthalten sein\n",
        "                filtered_data = filtered_data[\n",
        "                    filtered_data['clean_lyrics'].apply(lambda tokens: all(token in tokens for token in search_tokens))\n",
        "                ]\n",
        "            else:\n",
        "                print(\"Die Sucheingabe enthielt keine relevanten Wörter.\")\n",
        "\n",
        "        # Visualisierung der gefilterten Songs\n",
        "        visualize_emotions_and_display_songs(filtered_data)\n",
        "\n",
        "\n",
        "\n",
        "# Verknüpfung der Funktion mit dem Filter-Button\n",
        "filter_button.on_click(apply_filters)\n",
        "\n",
        "\n",
        "\n",
        "# Layout der Widgets\n",
        "layout = widgets.VBox([\n",
        "    widgets.Label(\"Filter songs based on emotions, artists, Spotify features, and lyrics:\"),\n",
        "    widgets.HBox([emotion_dropdown, artist_dropdown]),\n",
        "    valence_slider, danceability_slider, energy_slider, loudness_slider, tempo_slider,\n",
        "    lyrics_search, filter_button, output\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "# Anzeige des Layouts\n",
        "display(layout)\n"
      ],
      "metadata": {
        "id": "uFzVnv4KejoR",
        "outputId": "3919679b-7bb1-47b4-8005-9a01f10f7e96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5c6f7ef6fd964165af166cb3c6f8a0db",
            "5f0590e4073f441d8028ea1a07f3295e",
            "ad3a6533e62744b489271cc242b02297",
            "851530b898e4445db8de0659b4fc84e7",
            "892a82a2b49b4ca79e8a8c3b6eedb52b",
            "ba0f8d48367a4cf1b38be47f32f69e8b",
            "868d02761b84446e96bd7a5f23920857",
            "1eef487da6c9452595417f2ed5ecf648",
            "85d7d20363b747a8b3eb0e313189784a",
            "b8d15cf3915849db8708adbc20075b7e",
            "04a429d888af4b2295e42fe5e3f92df4",
            "deff9075a2c948a6a2e94353e26d9245",
            "e0379046b20f41658bb795abb57d64bc",
            "8cf84063825b41728645f1ab124dd154",
            "62a27141c7084c759090b69c05b8a83c",
            "da7bc7e2315645c78e6b6ea4f76ef1d0",
            "fdf6222161964f6d95acce4923bf0e21",
            "edbff8db938f44bfb700616789dfaf16",
            "ca8bf546549d4ac88d8e137f397b7aab",
            "ddf35a6d03f24ebb8854cba3aa924c1d",
            "d6d0e5a599924274b3e104b008e5d508",
            "5f88c65dce684e99a7dc41caddb01a5d",
            "4727b9e9259541c8ae714d2398437ec2",
            "b1f9c78f44b04d86a2be8ffdac7536d0",
            "f622a23aaf0a4622b84928ef66918a0c",
            "9634335eb84e498eb1918ef823e2e26a",
            "fef93ddd57d3423d95d1aadccf7ac123",
            "23d168c71877494690994f90634a2fa4",
            "cdc2604f39ea4fdcb8fcb50b4e81f937",
            "1596814f467c4d24a9f51c14c89efe31",
            "37717972938f4663b98a27561e66096b",
            "ef6c5878927845128b6c6f5e5e157afc",
            "728f4061b12f4a57a93a95aa096b9c45",
            "f62b517b7bfa453cbc63b564c6bad0ea",
            "df91946cf64a4e8f9f8bca5b4f66882c",
            "1e40dc07fd044d4fb4b5ac266d56dc21"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='Filter songs based on emotions, artists, Spotify features, and lyrics:'), HBox(chi…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c6f7ef6fd964165af166cb3c6f8a0db"
            }
          },
          "metadata": {}
        }
      ]
    }
  ]
}